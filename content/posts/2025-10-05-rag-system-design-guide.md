---
title: "RAG ì‹œìŠ¤í…œ ì„¤ê³„ ê°€ì´ë“œ: ê²€ìƒ‰ ì¦ê°• ìƒì„±ì˜ ì›ë¦¬ì™€ êµ¬í˜„"
date: 2025-10-05
draft: false
tags: ["RAG", "Vector Search", "Embedding", "LLM", "Retrieval"]
categories: ["AI Fundamentals"]
summary: "RAG(Retrieval-Augmented Generation)ì˜ í•µì‹¬ ì›ë¦¬, ì•„í‚¤í…ì²˜ íŒ¨í„´, ê·¸ë¦¬ê³  í”„ë¡œë•ì…˜ ë ˆë²¨ ì„¤ê³„ ì „ëµì„ ì •ë¦¬í•œë‹¤."
---

## RAGë€ ë¬´ì—‡ì¸ê°€

RAG(Retrieval-Augmented Generation)ëŠ” **ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•´ì„œ LLMì˜ ì‘ë‹µì— í™œìš©**í•˜ëŠ” ê¸°ìˆ ì´ë‹¤. LLMì˜ í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ” ì •ë³´(ì‚¬ë‚´ ë¬¸ì„œ, ìµœì‹  ë‰´ìŠ¤, ê°œì¸ ë°ì´í„°)ë¥¼ ë‹µë³€ì— í¬í•¨í•  ìˆ˜ ìˆê²Œ í•œë‹¤.

> "RAG is an AI framework for retrieving facts from an external knowledge base to ground large language models on the most accurate, up-to-date information."
> â€” [IBM What is RAG](https://www.ibm.com/think/topics/retrieval-augmented-generation)

```mermaid
flowchart LR
    subgraph Without["âŒ RAG ì—†ì´"]
        Q1["ì§ˆë¬¸"] --> L1["LLM"]
        L1 --> A1["í•™ìŠµ ë°ì´í„°ë§Œ<br/>í™œìš©í•œ ë‹µë³€"]
    end

    subgraph With["âœ… RAG ì‚¬ìš©"]
        Q2["ì§ˆë¬¸"] --> R["ê²€ìƒ‰"]
        R --> K["ê´€ë ¨ ì§€ì‹"]
        K --> L2["LLM + ì§€ì‹"]
        L2 --> A2["ê·¼ê±° ìˆëŠ” ë‹µë³€"]
    end

    style R fill:#E8F5E9
    style K fill:#E3F2FD
```

### RAGê°€ í•„ìš”í•œ ì´ìœ 

| ë¬¸ì œ | RAG ì—†ì´ | RAG ì‚¬ìš© |
|------|---------|---------|
| í™˜ê°(Hallucination) | LLMì´ ì‚¬ì‹¤ ì™œê³¡ | ê²€ì¦ëœ ì¶œì²˜ë¡œ ë‹µë³€ |
| ìµœì‹  ì •ë³´ | í•™ìŠµ ì´í›„ ì •ë³´ ì—†ìŒ | ì‹¤ì‹œê°„ ê²€ìƒ‰ ê°€ëŠ¥ |
| ë„ë©”ì¸ ì§€ì‹ | ì¼ë°˜ì  ë‹µë³€ë§Œ | ì‚¬ë‚´ ë¬¸ì„œ í™œìš© |
| ì¶œì²˜ íˆ¬ëª…ì„± | ê·¼ê±° ì—†ìŒ | ì°¸ì¡° ë¬¸ì„œ ëª…ì‹œ |

> "Three converging pressures make RAG essential in 2025: domain-specific accuracy, governance expectations, and the need for source traceability."
> â€” [RAG in 2025: From Quick Fix to Core Architecture](https://medium.com/@hrk84ya/rag-in-2025-from-quick-fix-to-core-architecture-9a9eb0a42493)

## RAG ì•„í‚¤í…ì²˜

RAGëŠ” í¬ê²Œ **Indexing(ì¸ë±ì‹±)**ê³¼ **Retrieval(ê²€ìƒ‰)** ë‘ ë‹¨ê³„ë¡œ ë‚˜ë‰œë‹¤.

```mermaid
flowchart TB
    subgraph Indexing["ğŸ“¥ Indexing (ì˜¤í”„ë¼ì¸)"]
        D["ë¬¸ì„œ"] --> C["ì²­í‚¹"]
        C --> E["ì„ë² ë”©"]
        E --> V["Vector DB"]
    end

    subgraph Retrieval["ğŸ” Retrieval (ì˜¨ë¼ì¸)"]
        Q["ì§ˆë¬¸"] --> QE["ì§ˆë¬¸ ì„ë² ë”©"]
        QE --> S["ìœ ì‚¬ë„ ê²€ìƒ‰"]
        V --> S
        S --> R["ê´€ë ¨ ì²­í¬"]
    end

    subgraph Generation["ğŸ’¬ Generation"]
        R --> P["í”„ë¡¬í”„íŠ¸ êµ¬ì„±"]
        Q --> P
        P --> L["LLM"]
        L --> A["ë‹µë³€"]
    end

    style E fill:#E3F2FD
    style S fill:#E8F5E9
    style L fill:#FFF3E0
```

## Step 1: ë¬¸ì„œ ì²­í‚¹ (Chunking)

ë¬¸ì„œë¥¼ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ê³¼ì •ì´ë‹¤. ì²­í‚¹ ì „ëµì´ RAG í’ˆì§ˆì˜ 70%ë¥¼ ê²°ì •í•œë‹¤.

> "Chunking is perhaps the most impactful decision in RAG system design. Poor chunking leads to poor retrieval, regardless of how good your embedding model is."
> â€” [Pinecone Chunking Strategies](https://www.pinecone.io/learn/chunking-strategies/)

### ì²­í‚¹ ì „ëµ

```mermaid
flowchart LR
    subgraph Fixed["ê³ ì • í¬ê¸°"]
        F1["500ì ë¶„í• "] ~~~ F2["ë¬¸ë§¥ ì†ì‹¤"]
    end

    subgraph Recursive["ì¬ê·€ì "]
        R1["ë¬¸ë‹¨â†’ë¬¸ì¥â†’ë‹¨ì–´"] ~~~ R2["êµ¬ì¡° ìœ ì§€"]
    end

    subgraph Semantic["ì‹œë§¨í‹±"]
        S1["ì˜ë¯¸ ë‹¨ìœ„"] ~~~ S2["ì„ë² ë”© í™œìš©"]
    end

    subgraph CodeAware["ì½”ë“œ ì¸ì‹"]
        C1["í•¨ìˆ˜/í´ë˜ìŠ¤"] ~~~ C2["AST"]
    end

    Fixed ~~~ Recursive ~~~ Semantic ~~~ CodeAware

    style Semantic fill:#E8F5E9
    style CodeAware fill:#E3F2FD
```

| ì „ëµ | ì¥ì  | ë‹¨ì  | ì í•©í•œ ìƒí™© |
|------|------|------|------------|
| ê³ ì • í¬ê¸° | êµ¬í˜„ ê°„ë‹¨ | ë¬¸ë§¥ ì†ì‹¤ | ë¹ ë¥¸ í”„ë¡œí† íƒ€ì… |
| ì¬ê·€ì  | êµ¬ì¡° ìœ ì§€ | ë¶ˆê· ë“± í¬ê¸° | ì¼ë°˜ ë¬¸ì„œ |
| ì‹œë§¨í‹± | ì˜ë¯¸ ë³´ì¡´ | ê³„ì‚° ë¹„ìš© | ê³ í’ˆì§ˆ í•„ìš” ì‹œ |
| ì½”ë“œ ì¸ì‹ | ì½”ë“œ êµ¬ì¡° ìœ ì§€ | ì–¸ì–´ë³„ êµ¬í˜„ í•„ìš” | ì½”ë“œë² ì´ìŠ¤ |

### ìµœì ì˜ ì²­í¬ í¬ê¸°

> "Best practices for RAG in 2025 recommend 400-512 tokens with 10-20% overlap."
> â€” [Firecrawl RAG Best Practices](https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025)

| íŒŒë¼ë¯¸í„° | ê¶Œì¥ê°’ | ì´ìœ  |
|----------|--------|------|
| ì²­í¬ í¬ê¸° | 400-512 í† í° | ì„ë² ë”© ëª¨ë¸ ìµœì  ë²”ìœ„ |
| ì˜¤ë²„ë© | 10-20% | ì²­í¬ ê²½ê³„ ë¬¸ë§¥ ë³´ì¡´ |
| ìµœì†Œ í¬ê¸° | 100 í† í° | ì˜ë¯¸ ì—†ëŠ” ì¡°ê° í•„í„°ë§ |

## Step 2: ì„ë² ë”© (Embedding)

í…ìŠ¤íŠ¸ë¥¼ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì´ë‹¤. **ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ëŠ” ê°€ê¹Œìš´ ë²¡í„°**ê°€ ëœë‹¤.

```mermaid
flowchart LR
    T1["'ê³ ì–‘ì´ê°€ ì”ë‹¤'"] --> V1["[0.8, 0.2]"]
    T2["'ê°•ì•„ì§€ê°€ ì”ë‹¤'"] --> V2["[0.7, 0.3]"]
    T3["'ìë™ì°¨ê°€ ë‹¬ë¦°ë‹¤'"] --> V3["[0.1, 0.9]"]

    V1 -.->|ìœ ì‚¬| V2
    V3 -.->|ê±°ë¦¬ ë©€ìŒ| V1

    style V1 fill:#E3F2FD
    style V2 fill:#E3F2FD
    style V3 fill:#FFEBEE
```

### ì„ë² ë”© ëª¨ë¸ ì„ íƒ

| ëª¨ë¸ | ì°¨ì› | íŠ¹ì§• | ë¹„ìš© |
|------|------|------|------|
| OpenAI text-embedding-3-large | 3072 | ê³ í’ˆì§ˆ, ë‹¤êµ­ì–´ | ìœ ë£Œ |
| Cohere embed-v3 | 1024 | ë‹¤êµ­ì–´ ê°•ì  | ìœ ë£Œ |
| qwen3-embedding | 1024 | MTEB 1ìœ„, ë¡œì»¬ | ë¬´ë£Œ |
| nomic-embed-text | 768 | ê°€ë²¼ì›€, ë¡œì»¬ | ë¬´ë£Œ |

> "For multilingual use cases, qwen3-embedding ranks #1 on MTEB Multilingual benchmark with 70.58 points."
> â€” [Ollama qwen3-embedding](https://ollama.com/library/qwen3-embedding)

### ë¡œì»¬ vs í´ë¼ìš°ë“œ

```mermaid
flowchart LR
    subgraph Cloud["â˜ï¸ í´ë¼ìš°ë“œ ì„ë² ë”©"]
        C1["OpenAI/Cohere<br/>âœ… ê³ í’ˆì§ˆ<br/>âŒ API ë¹„ìš©"]
    end

    subgraph Local["ğŸ  ë¡œì»¬ ì„ë² ë”©"]
        L1["Ollama/HuggingFace<br/>âœ… ë¹„ìš© ì—†ìŒ, ë³´ì•ˆ<br/>âš ï¸ GPU í•„ìš”"]
    end

    Cloud ~~~ Local

    style Cloud fill:#E3F2FD
    style Local fill:#E8F5E9
```

**ì„ íƒ ê¸°ì¤€:**

| ìƒí™© | ê¶Œì¥ |
|------|------|
| ì‚¬ë‚´ ì½”ë“œ/ë¬¸ì„œ | ë¡œì»¬ (ë³´ì•ˆ) |
| ê³µê°œ ë°ì´í„° | í´ë¼ìš°ë“œ (í’ˆì§ˆ) |
| ë¹„ìš© ë¯¼ê° | ë¡œì»¬ |
| ë¹ ë¥¸ êµ¬ì¶• | í´ë¼ìš°ë“œ |

## Step 3: ë²¡í„° ê²€ìƒ‰ (Vector Search)

ì§ˆë¬¸ ë²¡í„°ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ë²¡í„°ë¥¼ ì°¾ëŠ” ê³¼ì •ì´ë‹¤.

### ìœ ì‚¬ë„ ì¸¡ì •

```mermaid
flowchart LR
    M1["Cosine<br/>ë°©í–¥"] ~~~ M2["Euclidean<br/>ê±°ë¦¬"] ~~~ M3["Dot Product<br/>í¬ê¸°+ë°©í–¥"]

    style M1 fill:#E8F5E9
```

| ë°©ì‹ | ìˆ˜ì‹ | íŠ¹ì§• | ê¶Œì¥ ìƒí™© |
|------|------|------|----------|
| Cosine | cos(Î¸) | ì •ê·œí™”ëœ ë²¡í„°ì— ì í•© | ëŒ€ë¶€ë¶„ |
| Euclidean | âˆšÎ£(a-b)Â² | í¬ê¸° ì°¨ì´ ë°˜ì˜ | ê±°ë¦¬ ì¤‘ìš” ì‹œ |
| Dot Product | Î£(aÃ—b) | í¬ê¸° + ë°©í–¥ | ë¹„ì •ê·œí™” ë²¡í„° |

### ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤

| DB | íŠ¹ì§• | ì í•©í•œ ê·œëª¨ |
|----|------|------------|
| **Qdrant** | í•„í„°ë§ ê°•ë ¥, Rust ê¸°ë°˜ | ì¤‘~ëŒ€ê·œëª¨ |
| **Pinecone** | ê´€ë¦¬í˜•, ì‰¬ìš´ ì‹œì‘ | ëª¨ë“  ê·œëª¨ |
| **Chroma** | ê²½ëŸ‰, ì„ë² ë””ë“œ | ì†Œê·œëª¨ |
| **Milvus** | ëŒ€ê·œëª¨ ë¶„ì‚° | ëŒ€ê·œëª¨ |
| **pgvector** | PostgreSQL í™•ì¥ | ê¸°ì¡´ PG ì‚¬ìš© ì‹œ |

### ê²€ìƒ‰ ìµœì í™”

> "Vector-only retrieval is semantic and can miss exact tokens. Combine dense vectors for semantic recall with sparse/keyword fallback."
> â€” [RAG Best Practices](https://orkes.io/blog/rag-best-practices/)

**í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰:**

```mermaid
flowchart LR
    Q["ì§ˆë¬¸"] --> V["ë²¡í„° ê²€ìƒ‰"]
    Q --> K["í‚¤ì›Œë“œ ê²€ìƒ‰"]

    V --> R["Re-ranking"]
    K --> R

    R --> Top["Top-K"]

    style R fill:#FFF3E0
```

| ì „ëµ | íš¨ê³¼ |
|------|------|
| í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ | ì •í™•í•œ ìš©ì–´ + ì˜ë¯¸ ë§¤ì¹­ |
| Re-ranking | 1ì°¨ ê²€ìƒ‰ í›„ ì •êµí•œ ì¬ì •ë ¬ |
| ë©”íƒ€ë°ì´í„° í•„í„° | ë²”ìœ„ ì¶•ì†Œ (ë‚ ì§œ, ì¹´í…Œê³ ë¦¬) |
| MMR | ë‹¤ì–‘ì„± í™•ë³´ (ì¤‘ë³µ ì œê±°) |

## Step 4: í”„ë¡¬í”„íŠ¸ êµ¬ì„±

ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ LLM í”„ë¡¬í”„íŠ¸ì— íš¨ê³¼ì ìœ¼ë¡œ í†µí•©í•œë‹¤.

```mermaid
flowchart TB
    subgraph Prompt["í”„ë¡¬í”„íŠ¸ êµ¬ì„±"]
        S["ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"]
        C["ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸"]
        Q["ì‚¬ìš©ì ì§ˆë¬¸"]
    end

    S --> P["ìµœì¢… í”„ë¡¬í”„íŠ¸"]
    C --> P
    Q --> P
    P --> L["LLM"]

    style C fill:#E8F5E9
```

### í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì˜ˆì‹œ

```markdown
## ì§€ì‹œì‚¬í•­
ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.

## ì»¨í…ìŠ¤íŠ¸
{retrieved_chunks}

## ì§ˆë¬¸
{user_question}

## ë‹µë³€
```

### ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬

> "The key challenge in RAG is not finding relevant documents, but fitting them into the context window."
> â€” [Langchain RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/)

| ì „ëµ | ì„¤ëª… |
|------|------|
| í† í° ì˜ˆì‚° | ì»¨í…ìŠ¤íŠ¸ì— í• ë‹¹í•  ìµœëŒ€ í† í° ì„¤ì • |
| ì²­í¬ ì••ì¶• | ê¸´ ì²­í¬ë¥¼ ìš”ì•½í•´ì„œ í¬í•¨ |
| ìš°ì„ ìˆœìœ„ | ìœ ì‚¬ë„ ë†’ì€ ì²­í¬ ìš°ì„  í¬í•¨ |
| ì¤‘ë³µ ì œê±° | ìœ ì‚¬í•œ ì²­í¬ í†µí•© |

## ê³ ê¸‰ RAG íŒ¨í„´

### 1. Agentic RAG

ì—ì´ì „íŠ¸ê°€ ê²€ìƒ‰ì„ ë„êµ¬ë¡œ í™œìš©í•œë‹¤. í•„ìš”í•  ë•Œë§Œ ê²€ìƒ‰í•˜ê³ , ê²€ìƒ‰ ì¿¼ë¦¬ë„ ëŠ¥ë™ì ìœ¼ë¡œ ìƒì„±í•œë‹¤.

> "Agentic RAG treats retrieval as a tool that agents can invoke strategically."
> â€” [LlamaIndex Agentic RAG](https://www.llamaindex.ai/blog/agentic-rag)

```mermaid
flowchart TB
    A["ì—ì´ì „íŠ¸"] --> T{ê²€ìƒ‰ í•„ìš”?}
    T -->|Yes| R["ê²€ìƒ‰ ë„êµ¬ í˜¸ì¶œ"]
    T -->|No| D["ì§ì ‘ ë‹µë³€"]
    R --> E{ì¶©ë¶„?}
    E -->|No| Q["ì¿¼ë¦¬ ì¬êµ¬ì„±"]
    Q --> R
    E -->|Yes| D

    style A fill:#FCE4EC
    style R fill:#E8F5E9
```

### 2. Self-RAG

LLMì´ ìì²´ì ìœ¼ë¡œ ê²€ìƒ‰ í•„ìš”ì„±ì„ íŒë‹¨í•˜ê³ , ê²€ìƒ‰ ê²°ê³¼ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•œë‹¤.

| ë‹¨ê³„ | ë™ì‘ |
|------|------|
| 1. Retrieve? | ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ íŒë‹¨ |
| 2. Relevant? | ê²€ìƒ‰ ê²°ê³¼ ê´€ë ¨ì„± í‰ê°€ |
| 3. Supported? | ë‹µë³€ì´ ê²€ìƒ‰ ê²°ê³¼ì— ê·¼ê±°í•˜ëŠ”ì§€ í™•ì¸ |
| 4. Useful? | ìµœì¢… ë‹µë³€ ìœ ìš©ì„± í‰ê°€ |

### 3. Graph RAG

ë¬¸ì„œ ê°„ ê´€ê³„ë¥¼ ê·¸ë˜í”„ë¡œ ëª¨ë¸ë§í•˜ì—¬ ê²€ìƒ‰í•œë‹¤.

```mermaid
flowchart LR
    subgraph Graph["ì§€ì‹ ê·¸ë˜í”„"]
        A["ë¬¸ì„œ A"] -->|ì°¸ì¡°| B["ë¬¸ì„œ B"]
        B -->|ê´€ë ¨| C["ë¬¸ì„œ C"]
        A -->|ë™ì¼ ì£¼ì œ| C
    end

    Q["ì§ˆë¬¸"] --> Search["ê·¸ë˜í”„ íƒìƒ‰"]
    Search --> A & B & C

    style Graph fill:#E3F2FD
```

## RAG í‰ê°€ ì§€í‘œ

| ì§€í‘œ | ì¸¡ì • ëŒ€ìƒ | ëª©í‘œ |
|------|----------|------|
| **Precision@K** | ìƒìœ„ Kê°œ ì¤‘ ê´€ë ¨ ë¬¸ì„œ ë¹„ìœ¨ | ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ |
| **Recall@K** | ì „ì²´ ê´€ë ¨ ë¬¸ì„œ ì¤‘ ê²€ìƒ‰ëœ ë¹„ìœ¨ | ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ |
| **MRR** | ì²« ê´€ë ¨ ë¬¸ì„œ ìˆœìœ„ | ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ |
| **Faithfulness** | ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í•˜ëŠ”ì§€ | ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ |
| **Answer Relevance** | ë‹µë³€ì´ ì§ˆë¬¸ì— ì ì ˆí•œì§€ | ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ |

## ê²°ë¡ 

| í•µì‹¬ ê°œë… | ìš”ì•½ |
|----------|------|
| RAG | ì™¸ë¶€ ì§€ì‹ ê²€ìƒ‰ + LLM ìƒì„± |
| ì²­í‚¹ | í’ˆì§ˆì˜ 70% ê²°ì •, 400-512 í† í° ê¶Œì¥ |
| ì„ë² ë”© | ì˜ë¯¸ ê¸°ë°˜ ë²¡í„° ë³€í™˜ |
| ë²¡í„° ê²€ìƒ‰ | ìœ ì‚¬ë„ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ |
| í•˜ì´ë¸Œë¦¬ë“œ | ë²¡í„° + í‚¤ì›Œë“œ ê²°í•© |
| Agentic RAG | ì—ì´ì „íŠ¸ê°€ ê²€ìƒ‰ì„ ë„êµ¬ë¡œ í™œìš© |

RAGëŠ” LLMì˜ ê°€ì¥ í° ì•½ì ì¸ **í™˜ê°ê³¼ ì§€ì‹ í•œê³„**ë¥¼ í•´ê²°í•˜ëŠ” í•µì‹¬ ê¸°ìˆ ì´ë‹¤. ì¢‹ì€ RAG ì‹œìŠ¤í…œì„ ë§Œë“¤ë ¤ë©´ ì²­í‚¹ ì „ëµ, ì„ë² ë”© ëª¨ë¸ ì„ íƒ, ê²€ìƒ‰ ìµœì í™”ì— ì¶©ë¶„í•œ ì‹œê°„ì„ íˆ¬ìí•´ì•¼ í•œë‹¤.

## ì°¸ê³  ìë£Œ

- [What is RAG?](https://www.ibm.com/think/topics/retrieval-augmented-generation) - IBM
- [Chunking Strategies for LLM Applications](https://www.pinecone.io/learn/chunking-strategies/) - Pinecone
- [RAG Best Practices](https://orkes.io/blog/rag-best-practices/) - Orkes
- [Agentic RAG](https://www.llamaindex.ai/blog/agentic-rag) - LlamaIndex
- [RAG in 2025](https://medium.com/@hrk84ya/rag-in-2025-from-quick-fix-to-core-architecture-9a9eb0a42493) - Medium

---

> **ì´ì „ ê¸€**: [AI Agent ì•„í‚¤í…ì²˜ì˜ ì´í•´](/dev-notes/posts/2025-10-01-ai-agent-architecture-fundamentals/)
